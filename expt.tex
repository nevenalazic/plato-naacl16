\section{Experiments}
\label{sec:expt}

\subsection{Evaluation data}

\paragraph{CoNLL} 
The CoNLL dataset ~\cite{Hoffart2011} contains 1,393 articles with
about 34K mentions, and the standard performance metric is
mention-averaged accuracy.  The documents are partitioned into train,
test-a and test-b.  Most authors report performance on the subset of
231 test-b documents with 4,483 linkable mentions.

\paragraph{TAC KBP} 
The TAC KBP datasets \cite{TAC2010,TAC2011,TAC2012} include 2,250 mentions (2010),
2,250 mentions (2011), and 2,226 mentions (2012), 
of which roughly half are linkable
to the reference KB.  The competition evaluation includes $\NIL$
entities; participants are required to cluster $\NIL$ mentions across
documents so that all mentions of each unknown entity are assigned a
unique identifier.  For these datasets, we report in-KB accuracy,
overall accuracy (with all $\NIL$s in one cluster), and the competition
metric $B^{3+} F_1$ which evaluates $\NIL$ clustering.

\subsection{Experimental setup}

\subsubsection{Mention prior}

Our KB is derived from the Wikipedia subset of Freebase \cite{BollackerEPST08}, with about 4M entities. 
We use the KB to obtain a baseline prior over entities given a mention. As sources of alias counts, we use
Wikipedia page titles (including redirects and disambiguation pages), Freebase aliases, 
Wikipedia anchor text \todo{is this true -- we use wikilinks?},  and
 filtered anchor text from Web links into Wikipedia pages
\cite{singh12:wiki-links}.

As an optional additional source of aliases, we use the alias-entity mapping released
by \newcite{Hoffart2011}, obtained by extending the extending the
``means'' tables of YAGO \cite{hoffart2013yago2}. This mapping was released alonside
CoNLL gold entity annotations. When it was
published, it had 100\% mention recall on CoNLL, i.e. every annotated mention
could be mapped to at least one entity, and the set of entities included the gold entity. 
However, changes in canonical Wikipedia URLs, accented characters and
unicode usually result in mention losses over time, as not all URLs can be mapped to the KB.

For CoNLL only, we experiment with a third alias-entity mapping derived 
from \cite{Hoffart2011} by \newcite{Pershina2015}; we call it ``HP''.  
It is not known how candidates were pruned, but it has 100\% mention recall
and low ambiguity. For example, in the HP mapping the average ambiguity (number of
entities per alias) on CoNLL is only 13.3, compared to 55 in our KB mapping
and 69 in YAGO. Unsurprisingly, using only this source of aliases results in
high accuracy on CoNLL  \cite{Pershina2015,YamadaS0T16}.  

Table~\ref{tab:AliasTable} lists the statistics of the three alias-entity mappings
 and some of their combinations on the CoNLL test-b dataset. \todo{need mention recall and/or similar stats for TAC}

\begin{table}
  \centering
  \begin{tabular}{l|l|l|l|l}
    Alias & Mention &   Gold & Unique & Avg.  \\
    map          & recall  & recall &        & ambig. \\
    \hline
    KB     & 4318 & 4095 & 1055   & 55.7  \\
    YAGO & & & & \\
    *HP & 4477 & 4476   & 1224   & 13.3   \\
    KB+HP  & 4477 &  4477 & 955    & 60.9   \\
    all three& 4477    & 4477   & 879    & 79.4       
  \end{tabular}
  \caption{Alias-entity map statistics on CoNLL, test-b
    fold, 4483 gold mentions.  Mention recall is the number of
    mentions with at least one known entity; gold recall is the number
    of cases where the gold entity was included in the candidates.
    Unique aliases map to exactly one entity.  Ambiguity is averaged over
    mentions with at least one candidate. }
  \label{tab:AliasTable}
  % old KG
\end{table}


\subsubsection{Local and pairwise scores}
\label{sec:expt:features}

Our baseline system is similar in design and accuracy to Plato \cite{Lazic2015}.
Given the referrent phrase $m_i$ and textual context features ${\bf b}_i$, it computes
the probability of a candidate entity as $p_i(c) \propto p(c|m_i)p({\bf b}_i|c)$. 
The system resolves mentions independently and does not have an explicit coherence model;
however, it does capture some coherence information indirectly as referrent phrases are
included as string context features. We experiment with several versions of the
mention prior $p(c|m_i)$ as described in the previous section. \todo{But the system is 
retrained on different name priors, and so the context portion may change... do we say this / mention that it's semisupervised?}

% For standardized comparison we limit ourselves to
% candidates proposed by the baseline system.

\paragraph*{Scores for single-link model:}
In the \emph{single link} model, we simply set the local score for
mention $i$ and candidate $c$ to $s_i(c) = \ln p_i(c) - \ln (1 -
p_i(c))$ (i.e., log-odds), so that likely candidates get positive
scores.  We set the pairwise score between two candidates heuristically to
$s_{ij}(y_i, y_j) = \ln o(y_i, y_j) + 0.7$, where $o(y_i, y_j)$ is the number of
outlinks from the Wikipedia page of $y_i$ to the page of $y_j$.  We
consider up to three candidates for each mention; if the baseline
probability of the top candidate exceeds $0.9$, we only consider the top
candidate.\footnote{We have tried including more candidates, but the single link
almost never changes the decision for candidates with baseline score $p_i(c)>0.9$.}

\paragraph*{Scores for attention model:}
Local scores $s_i(y_i)$ for the attention model are derived from
$p_i(c)$.  As the attention models have no probabilistic
interpretation, we inject these local feature values:
\begin{itemize}
\item $1$ if $p_i(c)\ne 0$, and 0 otherwise
\item $\log p_i(c)$ if $p_i(c)>0$, and $0$ otherwise
\item $1$ if $p_i(c)=0$, and $0$ otherwise
\item $\log(1-p_i(c))$ if $p_i(c)<1$, and $1$ otherwise
\item $1$ if $p_i(c)=0$ and $0$ otherwise.
\end{itemize}
%
The third and fifth feature above address the case where the $\log$ is undefined. They effectively let us learn the value of $\log{0}$ for the
cases where $p_i(c)=0$ or $p_i(c)=1$.
%The weight vector $\ws$ is then learned to find best linear combination.  

%Edge scores $s_{ij}(y_i, y_j)$ are computed using the edge model $\wp$ and edge features between entity pairs:
\input{edge_features}


\begin{table}[ht!]
  \centering
  \begin{tabular}{l|l|r}
    System                 &  Alias map  & In-KB  \\
    & & accuracy \\
    \hline
    \newcite{Lazic2015} & N/A          & 86.4\% \\
    Chisholm et al. (2015) & YAGO                   & 88.7\% \\
    Our baseline           & KB+HP                & 90.2\% \\
    Single link            & KB                   & \todo{87.1\%} \\
    Attention              & KB+HP                & 91.5\% \\
    \hline
    Attention              & Only HP                & *92.84\% \\
    \newcite{Pershina2015} & Only HP                & *91.77\%
  \end{tabular}
\caption{CoNLL test-b evaluation for recent competitive systems and
  our models.  Accuracies obtained with alias-entity maps having very
  low average ambiguity are marked with `*'.  For details of
  alias-entity maps, see Table~\ref{tab:AliasTable} and text.}
 \label{table:conll_results} 
\end{table}

\begin{table}
\centering
%\small
\begin{tabular}{l|c|c|c}
 System/paper & In-KB & Overall & ${B^{3+}F_1}$ \\ 
 & acc. & acc. & \\
\hline
{Chisholm et al. 2015} & & & \\
  Baseline & & & \\
 Single link & & & \\
 Attention & & & \\
\hline \hline
\newcite{Cucerzan2011} & - & 86.8 &  {84.1} \\
\newcite{Lazic2015} & 79.3 & 86.5 & 84.0 \\
Single link & {\bf 81.2} & {\bf 87.0} & {\bf 84.5} \\
 Attention & & & \\
\hline
\hline
\newcite{Cucerzan2012}  R1 & 72.0 & 76.2 & 72.1  \\
\newcite{Cucerzan2012} R3 & 71.2 & {76.6} & {\bf 73.0} \\
\newcite{Lazic2015} & {74.2} & {76.6} & 71.2 \\
 Single link & {\bf 75.1} & {\bf 77.3} & {72.2} \\
 Attention & & & \\
\end{tabular}
\caption{Results on the TAC 2010 (top), TAC 2011 (middle), and TAC 2012 (bottom) evaluation datasets. All numbers are in
  percents. \label{table:tac_results} }
\end{table}

\subsection{Results}

Tables~\ref{table:conll_results} compares the performance of our model
and recent competitive systems on the CoNLL test-b benchmark
 in terms of mention-averaged accuracy. We also note the alias-entity
 mapping used in each system, as the corresponding gold recall is
 an upper bound on accuracy. Both the single-link and multi-focus attention models
 improve the performance over baseline system. In addition, the multi-focus model
 outperforms recent state-of-the-art models given the same candidate recall.

Table~\ref{table:tac_results} shows our results for the TAC KBP 2010, 2011, and 2012
evaluation datasets. To compute $\NIL$ clusters required for the overall accuracy
and $B^3+F1$, we simply rely on the fact that our KB is larger than the TAC
reference KB, similarly to previous work. We assign a unique $\NIL$ label to
all mentions of an entity that is in our KB but not in TAC. 
Once again, our attention models improve the performance over the baseline
system, with multi-focus attention outperforming single-link. In comparison to
prior work, we achieve competitive performance on TAC 2010 and the best
results to date on TAC 2011 and TAC 2012.


\subsection{Effect of $K$ on the attention model}
\input{dep_on_k}

\subsection{Effect of smoothing}
In \secref{sec:soft_attention} we proposed an extension of softmax smoothing to the $K$ attention case. In our experiments 
we cross-validated over a wide range of $\beta$ values, including $\beta=\infty$ which corresponds to taking
the exact sum of $K$ largest values. We found that the optimal value in most cases was  $\beta=\infty$ or large values
such as $\beta=10$ and $\beta=100$. This suggests that a {\em hard} attention model, where exactly $K$ mentions are picked is adequate in the current settings.

%\subsection{Examples of gains (and losses)}


%
%\section{Conclusion}
%\label{sec:End}
%
%We have described two new approaches to modeling coherence for entity
%resolution.  While most existing systems consider all relations
%between entity candidates for a document to assign a coherence score
%to a candidate, we use a novel attention mechanism to select the most
%relevant relations.  Our experimental results support \hl{do they?}
%the premise that the inclusion of all relations can hurt performance.
%Our models improve the performance of a baseline system on three
%evaluation benchmarks, and our best multi-focal attention model
%achieves state-of-the-art results on standard benchmarks against
%highly competitive recently built systems.



%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "main.tex"  ***
%%% tex-main-file: "main.tex"  ***
%%% End: ***
