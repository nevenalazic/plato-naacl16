\section{Conclusion}
\label{sec:End}

We have described an attention-based approach to collective entity resolution,
motivated by the observation that a non-salient entity in a long document may
 only have relations to a small subset of other entities. 
 We explored two approaches to attention: a multi-focus attention model
 with tractable inference decomposed over mentions, and a single-focus
model with global inference implemented using belief propagation.
%Two implementation
%of the approach were described: one which uses inference on star shaped graphs, and one
% which does global inference using belief propagation.
Our empirical results show that the methods results in significant performance gains
across several benchmarks. 

Experiments in varying the size of the attention beam $K$ in the star-shaped model suggest that
 multi-focus attention is beneficial.
 % and performs well when coupled with
 % simple tractable inference, decomposed over mentions. 
 It is of course possible to extend the global
 single-link model to the multi-focus case, by modifying the model
 factors and resulting messages. 
 However, the simplicity of the star-shaped model, its empirical effectiveness, and ease of learning parameters make it an attractive approach for easily incorporating attention into existing resolution models. The model can also readily be applied 
to other structured prediction problems in language processing, such as
selecting antecedents in coreference resolution.


Deep learning has recently been used in mutliple NLP applications, including parsing \cite{chen2014fast} and translation \cite{bahdanau2014neural}. 
Learning the local and pairwise scores in our model using a deep architecture rather
than a linear model would likely lead to performance improvements.
The star-shaped model is particularly amenable to this architecture, as it can be implemented via
a feed-forward sequence of operations (including sorting, which can be implemented with soft-max gates).

Finally, one may consider a more elaborate model in which attention
 depends on the current state of the system; 
 for example, the state can summarize the mention context.
%For example, this can allow the attention to change according to context. 
The dynamics of the underlying state can be modeled by
recurrent neural networks or LSTMs \cite{bahdanau2014neural}. 

In conclusion, we have shown that attention is an effective mechanism for improving entity resolution models, and that it can be implemented
via a simple inference mechanism, where model parameters can be easily learned. 
 


\comment{
 two new approaches to modeling coherence for entity
resolution.  While most existing systems consider all relations
between entity candidates for a document to assign a coherence score
to a candidate, we use a novel attention mechanism to select the most
relevant relations.  Our experimental results support \hl{do they?}
the premise that the inclusion of all relations can hurt performance.
Our models improve the performance of a baseline system on three
evaluation benchmarks, and our best multi-focal attention model
achieves state-of-the-art results on standard benchmarks against
highly competitive recently built systems.
}