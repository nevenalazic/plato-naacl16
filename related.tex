\section{Related work}
\label{sec:related}

\newcite{KBP2016ReadingList} and \newcite{ling2015design} provide
summaries of recent ER research.  Here we review work related to the
three main facets of our approach.


\subsection{Coherence scores}

Several systems \cite{Milne2008,KulkarniSRC09,Hoffart2011} use the
``Milne and Witten'' measure for relatedness between a pair of
entities, which is based on the number of Wikipedia articles citing
each entity page, and the number of articles citing both;
\newcite{Cucerzan07} has also relied on the Wikipedia category
structure.  Internal links from one entity page to another in
Wikipedia also provide direct evidence of relatedness between them.
Another (possibly more noisy) source of information are Web pages
containing links \cite{singh12:wiki-links} to Wikipedia pages of both
entities.  Such links have been used in several recent systems
\cite{ChengR13,Chisholm2015}.  \newcite{YamadaS0T16} train embedding
vectors for entities, and use them to define similarities.


\subsection{Collective inference for ER}

Optimizing most global coherence objectives is
intractable. \newcite{Milne2008} and \newcite{Ferragina10} decompose
the problem over mentions and select the candidate that maximizes
their relatedness score, which includes relations to all other
mentions.  \newcite{Hoffart2011} use
an iterative heuristic to remove unpromising
mention-entity edges.  \newcite{Cucerzan07} creates a relation vector
for each candidate, and disambiguates each entity to the candidate
whose vector is most similar to the aggregate (which includes both
correct and incorrect labels).  \newcite{ChengR13} use an integer
linear program solver and \newcite{KulkarniSRC09} use a convex
relaxation.  \newcite{Ratinov11} use relation scores as features in a
ranking SVM.  Belief propagation without attention has been used
by \newcite{ganea2015probabilistic}.
Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable
alternative, adopted by several recent systems
\cite{Han2011,He13,Alhelbawy14,Pershina2015}.
Laplacian smoothing \cite{Huang2014} is closely related.

% In most applications of PPR to entity resolution, the graph edges
% and weights are hardwired (not learned), and there is no dynamic
% association between $y_i$ and the best supporting mentions~$j$.
% Common shortcomings of these systems include the absence of a
% convincing generative or discriminative story that integrates local
% and coherence signals.


\subsection{Attention models}

Attention models have shown great promise in several applications,
including machine translation \cite{bahdanau2014neural} and image
caption generation \cite{xu2015show}.  We address a new application of
attention, and introduce a significantly different attention
mechanism, which allows each variable to focus on \emph{multiple}
objects.  We develop a novel smooth version of the multi-focus
attention function, which generalizes the single focus
softmax-function.  While some existing entity resolution systems
\cite{Jin:2014,Lazic2015} may be viewed as having attention
mechanisms, these are intended for single textual features and not
readily extensible to structured inference.

%Our work introduces a significantly different attention mechanism,
%which considers multiple foci of attention, and generalizes previous
%soft attention approach to handle those.  Furthermore, we optimize
%over the attention foci and the predicted label jointly, and not in a
%feed-forward manner as done in previous works.
%% We apply our new attention mechanism to coherence modeling for ER, and
%% we base the score for each candidate entity on relations to a small
%% subset of other mentions.  



%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "main.tex"  ***
%%% tex-main-file: "main.tex"  ***
%%% End: ***
