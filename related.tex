\section{Related work}
\label{sec:related}

\subsection{Coherence scores}

Several systems \cite{Milne2008,KulkarniSRC09,Hoffart2011} use the ``Milne and Witten'' measure for relatedness between a pair of entities, which is based on the number of Wikipedia articles containing each entity, and the number of articles containing both; \newcite{Cucerzan07} has also relied on the Wikipedia category structure. %The overall coherence score of a candidate is then computed as a weighted average of such similarities. 
Wikipedia also provides direct evidence of relatedness between a pair of of entities by way of intra-Wikipedia links from the page of one entity to another. Another source of information are Web pages containing links to Wikipedia pages of both entities (although the signal here may be more noisy); such links have been used in several recent systems \cite{ChengR13,Chisholm2015}.  Yet another attractive alternative, in few of the recent success of deep learning, is to build continuous vector representations of entities, and use those to define a pairwise similarity between entities \cite{YamadaS0T16}.


\subsection{Collective inference for ER}

As discussed earlier, optimizing most global coherence objectives is intractable. \newcite{Milne2008} and \newcite{Ferragina10} decompose the problem over mentions and select the candidate that maximizes their relatedness score, which includes relations to all other mentions. % but that involves extending attention to \emph{all} other mentions ($K=n-1$). 
\newcite{Hoffart2011} extract a dense subgraph from the original graph by using an iterative heuristic to remove unpromising mention-entity edges. \newcite{Cucerzan07} creates a relation vector for each candidate, and disambiguates each entity to the candidate whose vector is most similar to the aggregate (which includes both correct and incorrect labels). \newcite{KulkarniSRC09} formulate the task as an integer linear program and find solutions via a convex relaxation, while \newcite{ChengR13} directly use an integer linear program solver. 
%A shortcoming of these approaches is that they may be too slow \hl{check} for Web-scale ER.
\newcite{Ratinov11} use relation scores as features in a ranking support vector machine. 

Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable alternative to collective inference, adopted by several recent systems \cite{Han2011,He13,Alhelbawy14,Pershina2015}. A closely related approach is Laplacian smoothing \cite{Huang2014}. 
% In most applications of PPR to entity resolution, the graph edges and weights are fixed (not learned).
% and there is no dynamic association between $y_i$ and the best supporting mentions~$j$.

%Common shortcomings of these systems include the absence of a convincing generative or discriminative story that integrates local and coherence signals.  Typically, there is no training for the PPR part of the score; it is hardwired into the final labeling process. 

\subsection{Attention models}
Attention based models have recently shown great promise in several applications, including machine translation \cite{bahdanau2014neural} and image caption generation \cite{xu2015show}.  
%In both these domains, attention is used to choose which information is relevant for producing the next work. \todo{I don't understand this sentence}
Our work introduces a significantly different attention mechanism, which allows each variable can focus on multiple objects. 
We develop a novel smooth version of the multi-focus attention function which generalizes the single focus softmax-function.
%Our work introduces a significantly different attention mechanism, which considers multiple foci of attention, and generalizes previous soft attention approach to handle those. 
%Furthermore, we optimize
% over the attention foci and the predicted label jointly, and not in a feed-forward manner as done in previous works.

We apply the developed attention mechanism to coherence modeling for entity resolution, and we base the score for each candidate entity on relations to a small subset of mentions. While some existing entity resolution systems such as \newcite{Jin:2014} and \newcite{Lazic2015} may be viewed as having attention mechanisms, these are intended for single textual features and not readily extendible to mentions and structured inference.

