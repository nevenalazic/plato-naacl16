\section{Related work}
\label{sec:related}

\newcite{KBP2016ReadingList} and \newcite{ling2015design} provide
summaries of recent ER research.  Here we review work related to the
three main facets of our approach.


\subsection{Coherence scores}

Several systems \cite{Milne2008,KulkarniSRC09,Hoffart2011} use the
``Milne and Witten'' measure for relatedness between a pair of
entities, which is based on the number of Wikipedia articles citing
each entity page, and the number of articles citing both;
\newcite{Cucerzan07} has also relied on the Wikipedia category
structure.  Internal links from one entity page to another in
Wikipedia also provide direct evidence of relatedness between them.
Another (possibly more noisy) source of information are Web pages
containing links \cite{singh12:wiki-links} to Wikipedia pages of both
entities.  Such links have been used in several recent systems
\cite{ChengR13,Chisholm2015}.  \newcite{YamadaS0T16} build continuous
vector representations of entities, and use those to define pairwise
similarities. 


\subsection{Collective inference for ER}

As discussed earlier, optimizing most global coherence objectives is
intractable. \newcite{Milne2008} and \newcite{Ferragina10} decompose
the problem over mentions and select the candidate that maximizes
their relatedness score, which includes relations to all other
mentions.  \newcite{Hoffart2011} extract a dense subgraph from the
original graph by using an iterative heuristic to remove unpromising
mention-entity edges. \newcite{Cucerzan07} creates a relation vector
for each candidate, and disambiguates each entity to the candidate
whose vector is most similar to the aggregate (which includes both
correct and incorrect labels). \newcite{KulkarniSRC09} formulate the
task as an integer linear program (ILP) and find solutions via convex
relaxation, while \newcite{ChengR13} directly use an ILP solver.
\newcite{Ratinov11} use relation scores as features in a ranking
support vector machine.

Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable
alternative to collective inference, adopted by several recent systems
\cite{Han2011,He13,Alhelbawy14,Pershina2015}.  A closely related
approach is Laplacian smoothing \cite{Huang2014}.

% In most applications of PPR to entity resolution, the graph edges
% and weights are hardwired (not learned), and there is no dynamic
% association between $y_i$ and the best supporting mentions~$j$.
% Common shortcomings of these systems include the absence of a
% convincing generative or discriminative story that integrates local
% and coherence signals.


\subsection{Attention models}

Attention models have shown great promise in several applications,
including machine translation \cite{bahdanau2014neural} and image
caption generation \cite{xu2015show}.  We address a new application of
attention, and introduce a significantly different attention
mechanism, which allows each variable to focus on \emph{multiple}
objects.  We develop a novel smooth version of the multi-focus
attention function, which generalizes the single focus
softmax-function.  While some existing entity resolution systems
\cite{Jin:2014,Lazic2015} may be viewed as having attention
mechanisms, these are intended for single textual features and not
readily extensible to structured inference.

%Our work introduces a significantly different attention mechanism,
%which considers multiple foci of attention, and generalizes previous
%soft attention approach to handle those.  Furthermore, we optimize
%over the attention foci and the predicted label jointly, and not in a
%feed-forward manner as done in previous works.
%% We apply our new attention mechanism to coherence modeling for ER, and
%% we base the score for each candidate entity on relations to a small
%% subset of other mentions.  



%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "main.tex"  ***
%%% tex-main-file: "main.tex"  ***
%%% End: ***
