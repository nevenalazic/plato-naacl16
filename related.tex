
\section{Related work}
\label{sec:related}

\subsection{Coherence scores}

Several systems \cite{Milne2008,KulkarniSRC09,Hoffart2011} use the ``Milne and Witten'' measure for relatedness between a pair of entities, which is based on the number of Wikipedia articles containing each entity, and the number of articles containing both; \cite{Cucerzan07} has also relied on the Wikipedia category structure. %The overall coherence score of a candidate is then computed as a weighted average of such similarities. 
Wikipedia also provides direct evidence of relatedness between a pair of of entities by way of intra-Wikipedia links from the page of one entity to another. Another source of information are Web pages containing links to Wikipedia pages of both entities (although the signal here may be more noisy); such links have been used in several recent systems \cite{ChengR13,Chisholm2015}.  Yet another attractive alternative, in few of the recent success of deep learning, is to build continuous vector representations of entities, and use those to define a pairwise similarity between entities \hl{References for the embedding approach?}.


\subsection{Collective inference for ER}

As discussed earlier, optimizing most reasonable coherence objectives is intractable. \cite{Milne2008,Ferragina10} decompose the problem over mentions and select the candidate that maximizes their relatedness score\todo{this sounds like star / attention}.  
 \cite{Hoffart2011} extract a dense subgraph from the original graph by using an iterative heuristic to remove unpromising mention-entity edges. \cite{Cucerzan07} creates a relation vector for each candidate, and disambiguates each entity to the candidate whose vector was most similar to the aggregate (which includes both correct and incorrect labels). \newcite{KulkarniSRC09} formulate the task as an integer linear program and find solutions via a convex relaxation, while \newcite{ChengR13} directly use an integer linear program solver. A shortcoming of these approaches is that they may be too slow \hl{check} for Web-scale ER. \newcite{Ratinov11} use relation scores as features in a ranking support vector machine.
 
 Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable alternative to collective inference, adopted by several recent systems \cite{Han2011,He13,Alhelbawy14,Pershina2015}. A closely related approach is Laplacian smoothing \cite{Huang2014}.  
%Common shortcomings of these systems include the absence of a convincing generative or discriminative story that integrates local and coherence signals.  Typically, there is no training for the PPR part of the score; it is hardwired into the final labeling process. 

\subsection{Attention models}
Attention based models have recently shown great promise in several applications, such as machine translation and image caption generation. In both these domains, attention is used to choose which information is relevant for producing the next work. Our work introduces a significantly different attention mechanism, which considers multiple foci of attention, and generalizes previous soft attention approach to handle those. Furthermore, we optimize
over the attention foci and the predicted label jointly, and not in a feed-forward manner as done in previous works.

Here we consider attending to different mentions in a document. We note that systems such as \newcite{Jin:2014} and \newcite{Lazic2015} may be viewed as
attending to single context features, via a model that is very different from ours, and not readily extendible to attention to mentions.

%which perform per-instance feature selection and argue that many context features of a mention can be unnecessary or even detrimental. 

%\paragraph*{Heuristics:}
%\cite{Cucerzan07} avoided the combinatorial explosion by aggregating profile vectors representing all candidates for all mentions other than the one being labeled, and then choosing the entity label whose profile vector was most similar to the aggregate.   But the aggregates were therefore superpositions of profiles of correct and incorrect labels.  \cite{KulkarniSRC09} used hill-climbing.  \cite{Hoffart2011} also used an iterative heuristic to remove unpromising mention-entity edges and thus extract a dense subgraph from the original graph.

%\paragraph*{Optimization/learning:}
%\newcite{KulkarniSRC09} formulated the task as an integer linear program and found solutions via a convex relaxation, while \newcite{ChengR13} directly used an integer linear program solver.  These approaches worked in practical amounts of time, but were too slow \hl{check} for Web-scale NERD \cite{Lazic2015}.  \newcite{Ratinov11} use relation scores as features in a ranking support vector machine.

%\paragraph*{Personalized PageRank (PPR):}
%Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable alternative to (combinatorial) collective inference, adopted by several recent systems \cite{Han2011,He13,Alhelbawy14,Pershina2015}. A closely related approach is Laplacian smoothing \cite{Huang2014}.  Common shortcomings of these systems include the absence of a convincing generative or discriminative story that integrates local and coherence signals.  Typically, there is no training for the PPR part of the score; it is hardwired into the final labeling process. 



%Our approach to coherence is partly inspired by the systems reported by \cite{Jin:2014} and \cite{Lazic2015}, which perform per-instance feature selection and argue that many context features of a mention can be unnecessary or even detrimental. 