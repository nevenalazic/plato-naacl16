

\section{Single-link model}

In our first approach, which we name \emph{single link}, we consider a simple global objective function which maximizes over relation scores for each entity, rather than summing or averaging them:
\begin{align}
g^{SL}({\bf y}) &= \sum_i \bigg( s_i(y_i) + \max_{ j \neq i} s_{ij}(y_i, y_j) \bigg).
\end{align}
We consider relations between candidate entities to be directed, with asymmetric pairwise scores $s_{ij}(y_i, y_j)$, and rely on deterministic (not learned) scores. 
While exact inference in this model remains intractable, we can find approximate solutions using max-sum belief propagation. We provide the corresponding iterative message updates in Appendix A. 

\section{Attention model}

We now describe the our attention model for scoring relations. We first describe the optimization objective and inference, and then provide details on how scores are calculated and learned.
\subsection{Inference}
\subsubsection{Star model}
We start by considering a simple attention-free model in which inference is tractable, which we call a {\em star model}. Here, each mention $i$ is labeled independently of other mentions, based on a star-shaped graphical model centered at $i$, as illustrated in $\figref{fig:star}$. The corresponding score function $f_i$ is:
\be
f_i(y_1,\ldots,y_n) = \sum_j s_j(y_j) + \sum_{j\neq i} s_{ij}(y_i,y_j) ~,
\label{eq:star_obj}
\ee
and the label $y_i$ is obtained by maximizing over all labels, and returning that of $y_i$. Namely:
\be
y_i = \arg\max_{y_i} \max_{y_{j}: j\neq i} f_i(y_1,\ldots,y_n).
\ee
Due to the structure of $f_i$, maximization is easy and can be done in $O(nC^2)$, where $C$ is the maximum number of candidates.

\subsubsection{Adding attention \label{sec:add_attention}}
The score function in \eqref{eq:star_obj} aggregates scores from all other mentions $j\neq i$, in order to label mention $i$. If the document has many mentions, this can introduce noise into the decision process. To reduce this effect, we introduce attention into our model, and allow it to only use scores from mentions that have high coherence with $y_i$. We next define this notion more formally. 

We specify a parameter $K$ that denotes the maximum number of relations to consider for a candidate. Next, define the function $\amax(z_1,\ldots,z_n)$ to be
the sum of the top $K$ values in $z_1,\ldots,z_n$.  For each $j\neq i$, we define the coherence between label $y_i$ and mention $j$ to be:
\be
s_{ij}(y_i) = \max_{y_j}  s_{ij}(y_i,y_j)  + s_j(y_j),
\ee
and we also define $s_{ii}(y_i)=-\infty$ to simplify notation later. Thus, $s_{ij}(y_i)$ gives the strength of support that mention $j$ gives to candidate $y_i$. Since we want to only attend to the most informative mentions, we will only consider  the $K$ best mentions. Namely, we consider the score function:
\be
f_i(y_i) = s_i(y_i) + \amax(s_{i1}(y_i), \ldots, s_{in}(y_i))
\label{eq:amax_obj}
\ee
Labeling is then simply done by taking the argmax of $f_i(y_i)$. Computationally, the cost is $O(nC^2+ n\log{n})$ since sorting is required.\footnote{Note that if $k < \log{n}$, we can use $nk$ instead of $n\log{n}$.}  

\subsubsection{Soft Attention \label{sec:soft_attention}}
Several works on attention have shown that it is better to use a soft form of attention, where the level of attention is not zero or one, but can rather take intermediate values. In current attention models, the attention is to a single object (\hl{TODO: citations}e.g., a word as in \cite{} or part of an image as in \cite{}). In these cases, it is natural to change the max function in the attention operator to a soft-max. In our case, the attention beam contains $K$ elements, and we require a different notion of a soft-max, which we develop below.

Our goal is to obtain a soft-version of the function  $\amax(z_1,\ldots,z_n)$. To do so, we first use an alternative definition of this function, as the solution 
to the optimization problem:
\be
 \max_{ 
\begin{array}{l}
x_1,\ldots,x_n: \\
0 \leq x_i \leq 1\\
 \sum_i x_i = K
 \end{array}
 } \sum_i z_i x_i
\ee
The optimization problem above is a linear program, whose solution are the top $K$ elements of $z$ as required. This follows since optimal $x_i$ can easily be seen to attain only integral values (otherwise the objective can be improved). Alternatively, it can be shown that the vertices of the constraining polytope are integral.

Given this optimization view of $\amax(z_1,\ldots,z_n)$ it is natural to smooth it \cite{Nesterov} by adding a non-linearity to the optimization. A very natural choice
is an entropy regularizer, which also recovers the standard soft-max case for $K=1$.  Consider the optimization problem:
\be
 \max_{ 
\begin{array}{l}
x_1,\ldots,x_n: \\
0 \leq x_i \leq 1\\
 \sum_i x_i = K
 \end{array}
 } \sum_i z_i x_i + \beta \sum_i x_i \log{x_i}
\ee
We denote its solution by $\samax(z_1,\ldots,z_n)$. The following proposition provides a closed form solution for $\samax$, as well as its gradient.

\begin{proposition}
Assume $i_1,\ldots,i_n$ are such that $z_{i_1}\geq \ldots \geq z_{i_n}$. Then:
\be
\samax(z_1,\ldots,z_n) = \sum_{j=1}^{K-1} z_{i_j} + \beta \log\sum_{j=K} e^{\beta^{-1} z_{i_j}}  
\ee
Furthermore, $\samax(z_1,\ldots,z_n)$ is differentiable and its gradient is given by $....$ \hl{TODO}
\end{proposition}  
Proof is provided in the appendix.

As noted, $K=1$ recovers the standard soft-max function \cite{}. For other values of $K$, the function $\samax$ has an intuitive interpretation: take the sum of the top $K-1$ elements of $z_1,\ldots,z_n$, and add the soft-max of all other elements. As $\beta\to 0$, this soft-max will approach the true max and therefore the soft-max of the suffix will be exactly the $K^{th}$ top element, and $\samax$ will therefore be the sum of the top $K$ elements as expected. For finite $\beta$ we have a soft version of $\amax$.

Our soft attention based model will therefore consider the soft-variant of \eqref{eq:amax_obj} 
\be
f_i(y_i) = s_i(y_i) + \samax(s_{i1}(y_i), \ldots, s_{in}(y_i)) ~,
\label{eq:samax_obj}
\ee
and maximize $f(y_i)$ to obtain the label.
 
\subsection{Score Parameterization}
Thus far we assumed the singelton and pairwise scores were given. As in other structured prediction works, we will assume that the scores are features of the input and labels. Specifically, denote a set of singleton features by $\fs\in\reals^{n_s}$ and a set of pairwise features by $\fp\in\reals^{n_p}$. Then the model has two sets of weights $\ws$ and $\wp$ and the scores are obtained as a linear combination of the features. Namely:
\bea
s_i(y_i;\ws) &=& \ws\cdot\fs  \\
s_{ij}(y_i,y_j;\wp) &=& \wp\cdot\fp ~, \\
\eea
where we have expicitly denoted the dependence of the scores on the weight vectors. See \secref{sec:features} for details on how the features are chosen. It of course possible to consider non-linear alternatives for the score function, as in recent works \cite{manning,neurosis}, but we focus on the linear case for simplicity.

\subsection{Parameter Learning}
The parameters $\ws,\wp$ are learned from labeled data, as explained next. Since prediction is performed for each mention separately, we use a simple hinge loss for that mention. The loss is defined as follows. Denote by $y^*_i$ the ground
truth label for mention $i$. Define the corresponding ground truth $s_{ij}$ as:
\be
s^*_{ij} = s_{ij}(y^*_i,y^*_j)
\ee
Then the hinge loss is:
\bea
\ell_i &=& \max_{y_i}[ s_i(y_i) + \samax(s_{i1}(y_i), \ldots, s_{in}(y_i))  \\
       && - s_i(y^*_i) - \samax(s^*_{i1}, \ldots, s^*_{in})  
       + \Delta(y_i,y^*_i) ]
\eea
where $\Delta(y_i,y^*_i)$ is zero if $y_i=y^*_i$ and one otherwise.

The overall loss is simply the sum of loss for all the mentions, plus $\ell_2$ regularization over $\ws,\wp$. To minimize the loss we use AdaGrad \cite{adagrad} with learning rate $\eta=0.1$.

