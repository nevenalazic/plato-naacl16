\section{Introduction}
\label{sec:intro}

Named entity resolution and disambiguation (NERD) is the task of mapping each mention of an entity in a document to the corresponding record in a knowledge base (KB) \cite{BunescuP06,Cucerzan07,KulkarniSRC09,Dredze2010,Hoffart2011,Hachey2013130}.  
 %with numerous applications \cite{Gabrilovich2007,Lin2012,Kwiatkowski2011,finin2009Coreference,mayfield2009cross}. 
%Key applications 
%are to information extraction  and grounded semantic parsing. %text classification~\cite{Gabrilovich2007}, 
%information extraction~\cite{Lin2012} and grounded semantic parsing~\cite{Kwiatkowski2011}. 
%It can also provide a valuable signal to other language-processing tasks, including part-of-speech tagging, parsing, and coreference resolution~\cite{finin2009Coreference,mayfield2009cross}.
%Entity resolution
This is a challenging problem, as referring expressions are often ambiguous on their own, and can only be resolved given appropriate context. For example, the mention \qtext{Beirut} may refer to the capital of Lebanon, to the indie band from New Mexico, or to a drinking game. Names may also refer to entities that are not in the KB, a problem known as \emph{{\NIL} detection}. 
Entity resolution applications include numerous language-processing tasks  \cite{Gabrilovich2007,Lin2012,finin2009Coreference,mayfield2009cross}. %Kwiatkowski2011

Systems for entity resolution typically consist of a \emph{mention model}, a \emph{context model}, and a \emph{coherence model}. The mention model establishes a link between each entity and its textual representations, also called aliases or surface forms. The context model helps resolve an ambiguous phrase using textual features extracted from the surrounding context, such as the enclosing sentence and salient noun phrases in the document. The coherence model, which is our focus in this work, encourages all mentions to resolve to entities that are related to each other in the KB. 

Coherence models are often specified via a graph whose vertices are candidate entities for all mentions, and whose edges indicate known relations\footnote{An exception to this framework are topic models in which a topic may generate both entities and words, e.g. \cite{kataria2011,HanS12,houlsby2014scalable}.}.  Vertex weights correspond to prior scores for candidate entities, while edge weights correspond to some notion of relation strength. There exist different ways to leverage such a graph for inference purposes; most systems seek to maximize aggregate relatedness of each entity to all other entities \hl{reword?}. 


Our approach to coherence is partly inspired by the systems reported by \cite{Jin:2014} and \cite{Lazic2015}, which perform per-instance feature selection and argue that many context features of a mention can be unnecessary or even detrimental. 
Our premise is that this may also hold for entity relations: aggregating support for an entity label over the whole document may dilute the evidence for non-salient entities. We explore two new approaches to coherence that focus on a \emph{limited number} of relations for each candidate, rather than relations to all other entities. 

%  Accordingly, we propose to choose the label for each mention based on the best support from a \emph{limited number} of other mentions.  In other words,  each mention is labeled by (tractable) inference in a star graph, but one where most edges are (dynamically) ignored.

\subsection{Our contributions}
\label{sec:intro:our}


In the first approach, we formulate inference as finding the highest-weight subgraph in which each candidate has a directed edge to \emph{at most} one other candidate. This can roughly be seen as maximizing over neighboring edge weights as opposed to averaging; however, it is slightly more computationally involved since an edge between two entities is allowed only if the corresponding mentions resolve to them. We specify the objective and constraints using binary edge-indicator variables, and find the maximum-a-posteriori solution using the max-sum algorithm \cite{Kschischang2001}. 

In the second approach, \hl{TODO after reading again}.  %In other words,  each mention is labeled by (tractable) inference in a star graph, but one where most edges are (dynamically) ignored.

We use these techniques to re-score candidates generated by Plato \cite{Lazic2015}, a recent entity resolution system that has highly competitive performance and does not include a coherence component. This leads to performance improvements on three benchmarks, and yields new state-of-the-art results on the TAC KBP 2011 and 2012 datasets.

\hl{paper layout}


\section{Related work}
\label{sec:related}

Coherence among entities in a document has been exploited since early NERD papers \cite{Cucerzan07,Milne2008}.  
Various approaches differ in the manner in which they (a) compute relation scores, and (b) perform joint inference over entity labels to optimize coherence. 

\subsection{Coherence scores}

Several systems \cite{Milne2008,KulkarniSRC09,Hoffart2011} use the ``Milne and Witten'' measure for relatedness between a pair of entities, which is based on the number of Wikipedia articles containing each entity, and the number of articles containing both; \cite{Cucerzan07} has also relied on the Wikipedia category structure. %The overall coherence score of a candidate is then computed as a weighted average of such similarities. 
Wikipedia also provides direct evidence of relatedness between a pair of of entities by way of intra-Wikipedia links from the page of one entity to another. Another source of information are Web pages containing links to Wikipedia pages of both entities (although the signal here may be more noisy); such links have been used in several recent systems \cite{ChengR13,Chisholm2015}.  Yet another attractive alternative, in few of the recent success of deep learning, is to build continuous vector representations of entities, and use those to define a pairwise similarity between entities \hl{References for the embedding approach?}.



%relatedness measure \cite{Milne2008} or simpler or unweighted forms of it \cite{Cucerzan07}.  All of these used Wikipedia's category structure to characterize entity-to-entity similarity.  Several other signals have been used later.  Apart from categories, Wikipedia also provides direct evidence of relatedness between entities $c_1$ and $c_2$ by way of an intra-Wikipedia link from the page for $c_1$ to the page for $c_2$.  Further, links from a Web page to Wikipedia entity pages for entities $c_1, c_2$ signify relatedness between them (although the signal here may be more noisy).  Such links have been used in several systems \cite{ChengR13,Chisholm2015}.  Yet another attractive alternative, in few of the recent success of deep learning, is to build continuous vector representations of entities, and use those to define pairwise similarity between entities.


\subsection{Optimizing coherence}

There exist different ways to leverage the relation graph for disambiguation purposes.  For example, one may seek to maximize the average weight of edges connecting chosen entity nodes \cite{Milne2008,Ferragina10}.  However, optimizing most reasonable objectives is intractable.  %(If there are 10 mentions in a document, each with 5 entity candidates, then there are $5^{10}$ label combinations.)  %The main approaches to inference are discussed below.

 \cite{Hoffart2011} extract a dense subgraph from the original graph by using an iterative heuristic to remove unpromising mention-entity edges. \cite{Cucerzan07} creates a relation vector for each candidate, and disambiguates each entity to the candidate whose vector was most similar to the aggregate (which includes both correct and incorrect labels). \newcite{KulkarniSRC09} formulate the task as an integer linear program and find solutions via a convex relaxation, while \newcite{ChengR13} directly use an integer linear program solver. A shortcoming of these approaches is that they may be too slow \hl{check} for Web-scale NERD. \newcite{Ratinov11} use relation scores as features in a ranking support vector machine.
 
 Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable alternative to collective inference, adopted by several recent systems \cite{Han2011,He13,Alhelbawy14,Pershina2015}. A closely related approach is Laplacian smoothing \cite{Huang2014}.  
%Common shortcomings of these systems include the absence of a convincing generative or discriminative story that integrates local and coherence signals.  Typically, there is no training for the PPR part of the score; it is hardwired into the final labeling process. 

\hl{TODO: our approach in context of related work}


%\paragraph*{Heuristics:}
%\cite{Cucerzan07} avoided the combinatorial explosion by aggregating profile vectors representing all candidates for all mentions other than the one being labeled, and then choosing the entity label whose profile vector was most similar to the aggregate.   But the aggregates were therefore superpositions of profiles of correct and incorrect labels.  \cite{KulkarniSRC09} used hill-climbing.  \cite{Hoffart2011} also used an iterative heuristic to remove unpromising mention-entity edges and thus extract a dense subgraph from the original graph.

%\paragraph*{Optimization/learning:}
%\newcite{KulkarniSRC09} formulated the task as an integer linear program and found solutions via a convex relaxation, while \newcite{ChengR13} directly used an integer linear program solver.  These approaches worked in practical amounts of time, but were too slow \hl{check} for Web-scale NERD \cite{Lazic2015}.  \newcite{Ratinov11} use relation scores as features in a ranking support vector machine.

%\paragraph*{Personalized PageRank (PPR):}
%Personalized PageRank (PPR) \cite{jeh2003scaling} is another tractable alternative to (combinatorial) collective inference, adopted by several recent systems \cite{Han2011,He13,Alhelbawy14,Pershina2015}. A closely related approach is Laplacian smoothing \cite{Huang2014}.  Common shortcomings of these systems include the absence of a convincing generative or discriminative story that integrates local and coherence signals.  Typically, there is no training for the PPR part of the score; it is hardwired into the final labeling process. 


