\section{Introduction}
\label{sec:intro}

Entity resolution (ER) is the task of mapping each mention of an entity in a document to the corresponding record in a knowledge base (KB) \cite{BunescuP06,Cucerzan07,KulkarniSRC09,Dredze2010,Hoffart2011,Hachey2013130}.  
 %with numerous applications \cite{Gabrilovich2007,Lin2012,Kwiatkowski2011,finin2009Coreference,mayfield2009cross}. 
%Key applications 
%are to information extraction  and grounded semantic parsing. %text classification~\cite{Gabrilovich2007}, 
%information extraction~\cite{Lin2012} and grounded semantic parsing~\cite{Kwiatkowski2011}. 
%It can also provide a valuable signal to other language-processing tasks, including part-of-speech tagging, parsing, and coreference resolution~\cite{finin2009Coreference,mayfield2009cross}.
%Entity resolution
This is a challenging problem, as referring expressions are often ambiguous on their own, and can only be resolved given appropriate context. For example, the mention \qtext{Beirut} may refer to the capital of Lebanon, to the band from New Mexico, or to a drinking game. Names may also refer to entities that are not in the KB, a problem known as \emph{{\NIL} detection}. 
Applications of ER include numerous language-processing tasks, including text classification \cite{Gabrilovich2007}, information extraction \cite{Lin2012}, and coreference resolution \cite{finin2009Coreference,mayfield2009cross}. \todo{rephrase; too similar to Plato paper?}
%Kwiatkowski2011groundedsemparsing

Systems for entity resolution typically consist of a \emph{mention model}, a \emph{context model}, and a \emph{coherence model}~\cite{Milne2008,Cucerzan07,Ratinov11,Hoffart2011,Hachey2013130} \todo{refs taken straight from Plato}.  The mention model establishes a link between each entity and its textual representations, also called aliases or surface forms. The context model helps resolve an ambiguous phrase using textual features extracted from the surrounding context, such as the enclosing sentence and salient noun phrases in the document. The coherence model, which is our focus in this work, encourages all mentions to resolve to entities that are related to each other. Relations may be established via the KB, web links, or other resources. 

Coherence models often define an objective function that includes local and pairwise candidate scores, where the pairwise scores correspond to some notion of relation strength\footnote{An exception to this framework are topic models in which a topic may generate both entities and words, e.g. \cite{kataria2011,HanS12,houlsby2014scalable}.}. Finding an entity labeling that maximizes the objective is usually intractable and tackled via various approximations, which we discuss in more detail in Section \ref{sec:related}. One modeling observation is that salient or topical entities in a document can have many more relations than other entities. Thus, aggregating support for a non-salient entity over the entire document may dilute evidence and introduce noise, an issue we aim to address.

In this work, we introduce a novel coherence model with a multi-focal attention mechanism. In our model, the coherence score for each candidate entity depends on a small set of mentions it attends to, rather than all mentions in the document. Attention has recently been used with considerable empirical success in tasks such as translation \cite{bahdanau2014neural} and image caption generation \cite{xu2015show}. We argue that attention is also desirable for collective ER due to the discussed imbalance in the number of relations for different entities.

Our model relies on a novel smooth version of the multi-focus attention function, which generalizes the single-focus softmax function. We use a simple and efficient inference procedure, and show how the model parameters can be learned from data.

We evaluate our approach by adding coherence to the context model from the recently introduced Plato system \cite{Lazic2015} (which does not use a coherence model). This leads to performance improvements on three benchmarks, and yields new state-of-the-art results on the TAC KBP 2010-2012 datasets, and the CoNLL 2003 dataset.\todo{be more specific}

Our contributions thus consist of defining a novel multi-focal attention based model, where inference is efficient, and applying it successfully to an entity resolution system. We note that our model can be easily applied to other structured prediction problems in NLP.\todo{Consider expanding this}
%Our premise is that this may also hold for entity relations: aggregating support for an entity label over the whole document may dilute the evidence for non-salient entities. We explore two new approaches to coherence that focus on a \emph{limited number} of relations for each candidate, rather than relations to all other entities. 

%\input{fig_graph}
\begin{figure*}[th]
\centerline{
\includegraphics[width=0.5\linewidth]{beirut.pdf}
}
\caption{Illustration of the entity linking problem for the three mentions ``Beirut'', ``New Mexico'' and ``Santa Fe''. Each mention has three possible disambiguations. The links indicate disambiguations that have wikipedia links between their respective pages. }
\end{figure*}
%  Accordingly, we propose to choose the label for each mention based on the best support from a \emph{limited number} of other mentions.  In other words,  each mention is labeled by (tractable) inference in a star graph, but one where most edges are (dynamically) ignored.
\comment{
\subsection{Our contributions}
\label{sec:intro:our}


In the first approach, which we name \emph{single link}, we formulate inference as finding the highest-weight subgraph in which each candidate has a directed edge to \emph{at most} one other candidate. This can roughly be seen as maximizing over relations as opposed to averaging. In this model we perform inference using max-sum belief propagation \cite{Kschischang2001}.

%; however, it is slightly more computationally involved since an edge between two entities is allowed only if the corresponding mentions resolve to them. 
%We specify the objective and constraints using binary edge-indicator variables, and find the maximum-a-posteriori solution using the max-sum algorithm \cite{Kschischang2001}. 
Our

In the second \emph{attention} model, we allow each mention to have up to $K$ relations to other mentions. In this case, inference is tractable, and we also learn mention and edge scores from a small set of simple features.   %In other words,  each mention is labeled by (tractable) inference in a star graph, but one where most edges are (dynamically) ignored.

We use these coherence models to re-rank candidates generated by Plato \cite{Lazic2015}, a recent entity resolution system that has highly competitive performance and does not include a coherence component. This leads to performance improvements on three benchmarks, and yields new state-of-the-art results on the TAC KBP 2011 and 2012 datasets.
}
\todo{paper layout}


